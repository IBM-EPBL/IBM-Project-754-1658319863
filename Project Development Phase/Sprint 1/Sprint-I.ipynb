{"cells": [{"metadata": {}, "cell_type": "code", "source": "#IMPORTING THE NECESSARY LIBRARIES\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#CONNECTING DATASET WITH CODE\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\ncos_client = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='h1FPcWHr9FB2AeQQJsj776i_eopbZe6FfyOzYsA2PF2j',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n\nbucket = 'naturaldisasterintensityanalysisa-donotdelete-pr-pwiuxy2i5hirv2'\nobject_key = 'Cyclone_Wildfire_Flood_Earthquake_Database.zip'\n\nstreaming_body_2 = cos_client.get_object(Bucket=bucket, Key=object_key)['Body']\n\n# Your data file was loaded into a botocore.response.StreamingBody object.\n# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n# pandas documentation: http://pandas.pydata.org/\n", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#EXTRACTING THE DATASET USING BytesIO unzip function\nfrom io import BytesIO\nimport zipfile\nunzip=zipfile.ZipFile(BytesIO(streaming_body_2.read()),'r')\nfile_paths=unzip.namelist()\nfor path in file_paths:\n    unzip.extract(path)", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ls", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "\u001b[0m\u001b[01;34mCyclone_Wildfire_Flood_Earthquake_Database\u001b[0m/\r\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "pwd", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "'/home/wsuser/work'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "dir=r'/home/wsuser/work/Cyclone_Wildfire_Flood_Earthquake_Database'\n#CONFIGURING THE ImageDataGenerator CLASS\ntrain_datagen=ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\ntest_datagen=ImageDataGenerator(rescale=1./255)", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#LISTING OUT THE DISASTER CLASSES\nfor i in os.listdir(dir):\n    print(i)", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "readme.txt\nEarthquake\nCyclone\nWildfire\nFlood\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "path=os.path.join(dir,'readme.txt')\nos.remove(path)", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for i in os.listdir(dir):\n    print(i)", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "Earthquake\nCyclone\nWildfire\nFlood\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "pip install split_folders", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "Collecting split_folders\n  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\nInstalling collected packages: split-folders\nSuccessfully installed split-folders-0.5.1\nNote: you may need to restart the kernel to use updated packages.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "#SPLIT THE DATASET INTO TRAINING, TESTING AND VALIDATION DATA\nimport splitfolders\nsplitfolders.ratio(dir,output=\"dataset\",seed=42,ratio=(.7,.2,.1),group_prefix=None)", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "Copying files: 4428 files [00:02, 1678.84 files/s]\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "code", "source": "for i in os.listdir(dir):\n    print(i)", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "Earthquake\nCyclone\nWildfire\nFlood\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "dir1=r'/home/wsuser/work/dataset'", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for i in os.listdir(dir1):\n    print(i)", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "val\ntrain\ntest\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "#Apply ImageDataGenerator Functionality to Trainset and Testset(Validation Set)\nx_train = train_datagen.flow_from_directory(r\"/home/wsuser/work/dataset/train\",\n                                            target_size=(224,224),\n                                            batch_size=5,\n                                            color_mode='rgb',\n                                            class_mode='categorical')\nx_val=test_datagen.flow_from_directory(r\"/home/wsuser/work/dataset/val\",\n                                        target_size=(224,224),\n                                        batch_size=5,\n                                        color_mode='rgb',\n                                        class_mode='categorical')", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "Found 3097 images belonging to 4 classes.\nFound 884 images belonging to 4 classes.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "#70% of images goes to training, #20% of the total to validation, and #10% remaining goes to testing data\nx_test=test_datagen.flow_from_directory(r\"/home/wsuser/work/dataset/test\")\n                                        ", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "Found 447 images belonging to 4 classes.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}